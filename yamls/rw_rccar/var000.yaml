exp_name: rw_rccar/var000
seed: 1
log_level: debug

#################
### Algorithm ###
#################

alg:

  async:
    train:
      save_every_n_steps: 2.e+3 # about every 5 minutes
      reset_every_n_steps: # 1.e+5 # about every 50 minutes
  
    inference:
      ssh: "gkahn@ahsoka.banatao.berkeley.edu"
      remote_dir: "/home/gkahn/code/rllab/data/local"

      save_every_n_steps: 3.e+2 # for dt = 0.25, this is about 1.25 minutes

  ### Environment ###

  env: "RWrccarEnv(params={'collision_reward_only': True, 'collision_reward': -1, 'obs_shape': (V_height, V_width, V_color)})"
  env_eval: 
  normalize_env: False
  max_path_length: 1000
  n_envs: 1 # number of training environments
  render: False


  ### Offpolicy data ###

  offpolicy_rosbags:
    folders: ['/home/gkahn/code/rllab/data/local/rw-rccar/async1/rosbags', '/home/gkahn/code/rllab/data/local/rw-rccar/async2/rosbags', '/home/gkahn/code/rllab/data/local/rw-rccar/async3/rosbags', '/home/gkahn/code/rllab/data/local/rw-rccar/async4/rosbags']
    total_steps: 4.e+5
    save_every_n_steps: 1.e+4
    update_target_every_n_steps: 1.e+3
    log_every_n_steps: 1.e+3
  offpolicy:
  num_offpolicy: # number of offpolicy datapoints to load


  ### Steps ###

  total_steps: 14.4e+3 # 1 hours worth, corresponding to number of env.step(...) calls

  sample_after_n_steps: -1
  onpolicy_after_n_steps: 1.e+3 # take random actions until this many steps is reached

  learn_after_n_steps: -1 # when to start training the model
  train_every_n_steps: 0.25 # number of calls to model.train per env.step (if fractional, multiple trains per step)
  eval_every_n_steps: 5.e+2 # how often to evaluate policy in env_eval

  update_target_after_n_steps: -1 # after which the target network can be updated
  update_target_every_n_steps: 5.e+3 # how often to update target network
  update_preprocess_every_n_steps: 1.e+3 # how often to update preprocess (see preprocess below)

  save_every_n_steps: 1.e+3 # how often to save experiment data
  log_every_n_steps: 1.e+3 # how often to print log information


  ### Exploration ###

  exploration_strategies:
    # endpoints: [[step, value], [step, value], ...]
    GaussianStrategy: # additive gaussian noise
      endpoints: [[0, 1.0], [9.6e+3, 0.1], [14.4e+3, 0.01]]
      outside_value: 0.01
    EpsilonGreedyStrategy:
      endpoints: [[0, 1.0], [9.6e+3, 0.1], [14.4e+3, 0.01]]
      outside_value: 0.01


  ### Replay pool

  batch_size: 32 # per training step
  replay_pool_size: 1.e+6
  replay_pool_sampling: uniform # <uniform/terminal>
  replay_pool_params:
    terminal:
      frac: 0.5 # fraction of batch that from an end of an episode


  ### Saving data

  save_rollouts: True
  save_rollouts_observations: True # False saves space
  save_env_infos: False # False saves space

##############
### Policy ###
##############

policy:
  N: V_H # label horizon
  H: V_H # model horizon
  gamma: 0.99 # discount factor
  obs_history_len: 4 # number of previous observations to concatenate (inclusive)

  values_softmax: # how to weight the output predictions from horizons [1..H]
    type: mean # <mean/final/exponential>
    exponential:
      lambda: 0.9
  use_target: False # target network?
  separate_target_params: True # if target network, separate parameters?
  clip_cost_target_with_dones: False # if False, extends end of episodes by assuming 0 rewards and random actions taken

  get_action_test: # how to select actions at test time (i.e., when gathering samples)
    H: V_H
    values_softmax:
      type: mean # <mean/final/exponential>
      exponential:
        lambda: 0.9
    type: random # <random/lattice> action selection method
    random:
      K: V_K
    lattice:

  get_action_target: # for computing target values
    H: V_H
    values_softmax:
      type: mean # <mean/final/exponential>
      exponential:
        lambda: 0.9
    type: random # <random/lattice>
    random:
      K: 100
    lattice:

  class: ProbcollGCGPolicy # <MACPolicy/RCcarMACPolicy> model class
  GCGPolicy: &idGCGPolicy # outputs values
    image_graph: # CNN
      filters: [64, 32, 32, 32]
      kernels: [8, 4, 3, 3]
      strides: [4, 2, 2, 2]
      padding: SAME
      conv_activation: relu # <relu>
      output_activation: relu # <relu/tanh/sigmoid/softmax>
      normalizer: # <layer_norm/weight_norm/batch_norm>
      batch_norm_decay: 0.9

    observation_graph: # fully connected
      hidden_layers: [256]
      hidden_activation: relu # <relu/tanh>
      output_dim: 128 # this is the hidden size of the rnn
      output_activation: relu # <relu/tanh/sigmoid/softmax>
      normalizer: # <layer_norm/weight_norm/batch_norm>
      batch_norm_decay: 0.9

    action_graph: # fully connected
      hidden_layers: [16]
      hidden_activation: relu # <relu/tanh>
      output_dim: 16
      output_activation: relu # <relu/tanh/sigmoid/softmax>
      normalizer: # <layer_norm/weight_norm/batch_norm>
      batch_norm_decay: 0.9

    rnn_graph:
      num_cells: 1
      cell_type: mulint_lstm # <rnn/mulint_rnn/lstm/mulint_lstm>
      cell_args: # If you need to pass variables to cells
        use_layer_norm: False

    output_graph: # fully connected
      hidden_layers: [16]
      hidden_activation: relu # <relu/tanh>
      normalizer: # <layer_norm/weight_norm/batch_norm>
      batch_norm_decay: 0.9

    only_completed_episodes: False # only train with fully completed episodes?
  ProbcollGCGPolicy: # outputs collision probabilities
    <<: *idGCGPolicy
    speed_weight: 0. # how much do we care about going the maximum speed possible? (doesn't matter when fixed speed environment)
    probcoll_strictly_increasing: False # enforce predicted probabilities always increase
    is_classification: True # use cross entropy loss instead of mean squared error
    coll_weight_pct: # if replay_pool_sample is terminal, how much to reweight collision vs non-collision trajectories in the cost function?

  # preprocessing
  preprocess: # whiten observations / actions / rewards
    observations_im_mean: True
    observations_im_orth: True
    observations_vec_mean: False
    observations_vec_orth: False
    actions_mean: False
    actions_orth: False
    rewards_mean: False
    rewards_orth: False

  # training
  weight_decay: 1.e-6 # L2 regularization
  lr_schedule: # learning rate schedule
    endpoints: [[0, 1.e-4], [1.e+6, 1.e-4]]
    outside_value: 1.e-4
  grad_clip_norm: 10 # clip the gradient magnitude

  # device
  gpu_device: V_device
  gpu_frac: V_frac

